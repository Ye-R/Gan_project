{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e827752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SDML\\anaconda3\\envs\\gan1\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\SDML\\anaconda3\\envs\\gan1\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\SDML\\anaconda3\\envs\\gan1\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import scipy\n",
    "from scipy.spatial.distance import mahalanobis as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7334bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats       as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold      import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f669c",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3bd8cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "SensorList = [('Current'),('Acc'),('Vol')]\n",
    "Type = 'Misalign'\n",
    "raw_dir ='C:/Users/SDML/1GAN/%s'%Type  #raw 위치 확인!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "420db1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(x):\n",
    "    return np.sqrt(np.mean(x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "536606a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SensorIndex():\n",
    "    if Sensor == 'Current':\n",
    "        SensorIndex = 3\n",
    "    elif Sensor == 'Vol':\n",
    "        SensorIndex = 4\n",
    "    elif Sensor == 'Acc':\n",
    "        SensorIndex = 5\n",
    "    return SensorIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "504abe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataSet_raw(raw_dir):\n",
    "    x_train = []\n",
    "    path_dir = raw_dir\n",
    "    file_list = os.listdir(path_dir)\n",
    "    for i in range(len(file_list)):\n",
    "        path = path_dir + '/' + file_list[i]\n",
    "        data = pd.read_csv(path, sep=',', header=None).iloc[:,SensorIndex]\n",
    "        \n",
    "        x_train.append(data)\n",
    "        \n",
    "    x_train = np.array(x_train, dtype = 'float32')\n",
    "    \n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdff495",
   "metadata": {},
   "source": [
    "# Outlier 제끼기(모든 케이스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22c4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84daa45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Type = 'Normal'\n",
    "if Sensor == 'Current':\n",
    "    SensorIndex = 3\n",
    "elif Sensor == 'Vol':\n",
    "    SensorIndex = 4\n",
    "elif Sensor == 'Acc':\n",
    "    SensorIndex = 5\n",
    "\n",
    "RawData = load_dataSet_raw(raw_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43d955ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487, 2774)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RawData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe135b",
   "metadata": {},
   "source": [
    "## 삭제하는 인덱스 고르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98a72826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in range (0,3):\n",
    "    Sensor = SensorList[x]\n",
    "    \n",
    "    if Sensor == 'Current':\n",
    "        SensorIndex = 3\n",
    "    elif Sensor == 'Vol':\n",
    "        SensorIndex = 4\n",
    "    elif Sensor == 'Acc':\n",
    "        SensorIndex = 5\n",
    "\n",
    "\n",
    "    RawData = load_dataSet_raw(raw_dir)\n",
    "    RawData_Max = np.zeros((RawData.shape[0],1))\n",
    "    RawData_Min = np.zeros((RawData.shape[0],1))\n",
    "\n",
    "    for i in range(RawData.shape[0]):\n",
    "        RawData_Max[i] = np.max(RawData[i,:])\n",
    "        RawData_Min[i] = np.min(RawData[i,:])\n",
    "\n",
    "    Top10_RawData = np.percentile(RawData_Max,95)\n",
    "    Low10_RawData = np.percentile(RawData_Min,5)\n",
    "\n",
    "    loc_top10 = np.where(RawData_Max >= Top10_RawData)[0]\n",
    "    loc_low10 = np.where(RawData_Min <= Low10_RawData)[0]\n",
    "\n",
    "    delete_index_ = np.append(loc_top10,loc_low10)\n",
    "    s = 'delete_index_%s = np.unique(delete_index_)'%(Sensor)\n",
    "    exec(s)\n",
    "\n",
    "delete_index_repeat = np.append(np.append(delete_index_Current, delete_index_Vol), delete_index_Acc)\n",
    "delete_index = np.unique(delete_index_repeat)\n",
    "delete_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ecf9c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  27,  28,  29,  30,  32,  48,  50,  57,  63,  64,  68,  69,\n",
       "        70,  71,  72,  73,  75,  78,  81,  84,  93,  94,  97, 108, 109,\n",
       "       117, 119, 121, 122, 135, 136, 141, 143, 147, 149, 151, 153, 155,\n",
       "       158, 161, 162, 163, 166, 170, 171, 175, 180, 183, 202, 207, 210,\n",
       "       213, 214, 216, 217, 220, 221, 224, 225, 226, 228, 229, 231, 238,\n",
       "       239, 244, 251, 258, 271, 277, 284, 291, 297, 304, 305, 310, 312,\n",
       "       313, 316, 318, 319, 323, 327, 334, 345, 356, 372, 401, 405, 406,\n",
       "       421, 432, 441, 442, 449, 450, 454, 462, 463, 465, 474, 475, 477,\n",
       "       480], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6d421f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    path_dir = raw_dir\n",
    "    file_list = os.listdir(path_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "331f5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "index =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad3eebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misalign_102.csv\n",
      "Misalign_123.csv\n",
      "Misalign_124.csv\n",
      "Misalign_125.csv\n",
      "Misalign_126.csv\n",
      "Misalign_128.csv\n",
      "Misalign_142.csv\n",
      "Misalign_144.csv\n",
      "Misalign_150.csv\n",
      "Misalign_156.csv\n",
      "Misalign_157.csv\n",
      "Misalign_160.csv\n",
      "Misalign_161.csv\n",
      "Misalign_162.csv\n",
      "Misalign_163.csv\n",
      "Misalign_164.csv\n",
      "Misalign_165.csv\n",
      "Misalign_167.csv\n",
      "Misalign_17.csv\n",
      "Misalign_172.csv\n",
      "Misalign_175.csv\n",
      "Misalign_183.csv\n",
      "Misalign_184.csv\n",
      "Misalign_187.csv\n",
      "Misalign_197.csv\n",
      "Misalign_198.csv\n",
      "Misalign_204.csv\n",
      "Misalign_206.csv\n",
      "Misalign_208.csv\n",
      "Misalign_209.csv\n",
      "Misalign_220.csv\n",
      "Misalign_221.csv\n",
      "Misalign_226.csv\n",
      "Misalign_228.csv\n",
      "Misalign_231.csv\n",
      "Misalign_233.csv\n",
      "Misalign_235.csv\n",
      "Misalign_237.csv\n",
      "Misalign_239.csv\n",
      "Misalign_241.csv\n",
      "Misalign_244.csv\n",
      "Misalign_245.csv\n",
      "Misalign_246.csv\n",
      "Misalign_249.csv\n",
      "Misalign_252.csv\n",
      "Misalign_253.csv\n",
      "Misalign_257.csv\n",
      "Misalign_261.csv\n",
      "Misalign_264.csv\n",
      "Misalign_281.csv\n",
      "Misalign_286.csv\n",
      "Misalign_289.csv\n",
      "Misalign_291.csv\n",
      "Misalign_292.csv\n",
      "Misalign_294.csv\n",
      "Misalign_295.csv\n",
      "Misalign_298.csv\n",
      "Misalign_299.csv\n",
      "Misalign_300.csv\n",
      "Misalign_301.csv\n",
      "Misalign_302.csv\n",
      "Misalign_304.csv\n",
      "Misalign_305.csv\n",
      "Misalign_307.csv\n",
      "Misalign_313.csv\n",
      "Misalign_314.csv\n",
      "Misalign_319.csv\n",
      "Misalign_325.csv\n",
      "Misalign_331.csv\n",
      "Misalign_343.csv\n",
      "Misalign_349.csv\n",
      "Misalign_355.csv\n",
      "Misalign_361.csv\n",
      "Misalign_367.csv\n",
      "Misalign_373.csv\n",
      "Misalign_374.csv\n",
      "Misalign_379.csv\n",
      "Misalign_380.csv\n",
      "Misalign_381.csv\n",
      "Misalign_384.csv\n",
      "Misalign_386.csv\n",
      "Misalign_387.csv\n",
      "Misalign_390.csv\n",
      "Misalign_394.csv\n",
      "Misalign_40.csv\n",
      "Misalign_41.csv\n",
      "Misalign_42.csv\n",
      "Misalign_434.csv\n",
      "Misalign_460.csv\n",
      "Misalign_464.csv\n",
      "Misalign_465.csv\n",
      "Misalign_479.csv\n",
      "Misalign_5.csv\n",
      "Misalign_58.csv\n",
      "Misalign_59.csv\n",
      "Misalign_65.csv\n",
      "Misalign_66.csv\n",
      "Misalign_7.csv\n",
      "Misalign_77.csv\n",
      "Misalign_78.csv\n",
      "Misalign_8.csv\n",
      "Misalign_88.csv\n",
      "Misalign_89.csv\n",
      "Misalign_90.csv\n",
      "Misalign_93.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(file_list)):\n",
    "    for k in delete_index:\n",
    "        if(i==k):\n",
    "            print(file_list[i])\n",
    "            index = np.append(index,file_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c65bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(file_list)):\n",
    "    for k in delete_index:\n",
    "        if(file_list[i]==k):\n",
    "            print(file_list[i])\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99d2bd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Misalign_102.csv', 'Misalign_123.csv', 'Misalign_124.csv',\n",
       "       'Misalign_125.csv', 'Misalign_126.csv', 'Misalign_128.csv',\n",
       "       'Misalign_142.csv', 'Misalign_144.csv', 'Misalign_150.csv',\n",
       "       'Misalign_156.csv', 'Misalign_157.csv', 'Misalign_160.csv',\n",
       "       'Misalign_161.csv', 'Misalign_162.csv', 'Misalign_163.csv',\n",
       "       'Misalign_164.csv', 'Misalign_165.csv', 'Misalign_167.csv',\n",
       "       'Misalign_17.csv', 'Misalign_172.csv', 'Misalign_175.csv',\n",
       "       'Misalign_183.csv', 'Misalign_184.csv', 'Misalign_187.csv',\n",
       "       'Misalign_197.csv', 'Misalign_198.csv', 'Misalign_204.csv',\n",
       "       'Misalign_206.csv', 'Misalign_208.csv', 'Misalign_209.csv',\n",
       "       'Misalign_220.csv', 'Misalign_221.csv', 'Misalign_226.csv',\n",
       "       'Misalign_228.csv', 'Misalign_231.csv', 'Misalign_233.csv',\n",
       "       'Misalign_235.csv', 'Misalign_237.csv', 'Misalign_239.csv',\n",
       "       'Misalign_241.csv', 'Misalign_244.csv', 'Misalign_245.csv',\n",
       "       'Misalign_246.csv', 'Misalign_249.csv', 'Misalign_252.csv',\n",
       "       'Misalign_253.csv', 'Misalign_257.csv', 'Misalign_261.csv',\n",
       "       'Misalign_264.csv', 'Misalign_281.csv', 'Misalign_286.csv',\n",
       "       'Misalign_289.csv', 'Misalign_291.csv', 'Misalign_292.csv',\n",
       "       'Misalign_294.csv', 'Misalign_295.csv', 'Misalign_298.csv',\n",
       "       'Misalign_299.csv', 'Misalign_300.csv', 'Misalign_301.csv',\n",
       "       'Misalign_302.csv', 'Misalign_304.csv', 'Misalign_305.csv',\n",
       "       'Misalign_307.csv', 'Misalign_313.csv', 'Misalign_314.csv',\n",
       "       'Misalign_319.csv', 'Misalign_325.csv', 'Misalign_331.csv',\n",
       "       'Misalign_343.csv', 'Misalign_349.csv', 'Misalign_355.csv',\n",
       "       'Misalign_361.csv', 'Misalign_367.csv', 'Misalign_373.csv',\n",
       "       'Misalign_374.csv', 'Misalign_379.csv', 'Misalign_380.csv',\n",
       "       'Misalign_381.csv', 'Misalign_384.csv', 'Misalign_386.csv',\n",
       "       'Misalign_387.csv', 'Misalign_390.csv', 'Misalign_394.csv',\n",
       "       'Misalign_40.csv', 'Misalign_41.csv', 'Misalign_42.csv',\n",
       "       'Misalign_434.csv', 'Misalign_460.csv', 'Misalign_464.csv',\n",
       "       'Misalign_465.csv', 'Misalign_479.csv', 'Misalign_5.csv',\n",
       "       'Misalign_58.csv', 'Misalign_59.csv', 'Misalign_65.csv',\n",
       "       'Misalign_66.csv', 'Misalign_7.csv', 'Misalign_77.csv',\n",
       "       'Misalign_78.csv', 'Misalign_8.csv', 'Misalign_88.csv',\n",
       "       'Misalign_89.csv', 'Misalign_90.csv', 'Misalign_93.csv'],\n",
       "      dtype='<U32')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range (0,3):\n",
    "    Sensor = SensorList[x]\n",
    "    \n",
    "    if Sensor == 'Current':\n",
    "        SensorIndex = 3\n",
    "    elif Sensor == 'Vol':\n",
    "        SensorIndex = 4\n",
    "    elif Sensor == 'Acc':\n",
    "        SensorIndex = 5\n",
    "        \n",
    "    raw_dir ='C:/Users/SDML/SDML challenge/%s'%Type\n",
    "    RawData = load_dataSet_raw(raw_dir)\n",
    "    Data  = np.transpose(np.delete(RawData, delete_index, axis = 0))\n",
    "\n",
    "    NoOfData = Data.shape[1]\n",
    "    NoOfSensor  = 1\n",
    "    NoOfFeature = 10\n",
    "\n",
    "\n",
    "    Real_TimeFeature  = np.zeros((NoOfSensor*NoOfFeature , NoOfData))\n",
    "\n",
    "\n",
    "    for i in range(NoOfData):\n",
    "\n",
    "        # Normal Time Domain Feature\n",
    "        Real_TimeFeature[0, i] = np.max(Data[:,i])\n",
    "        Real_TimeFeature[1, i] = np.min(Data[:,i])\n",
    "        Real_TimeFeature[2, i] = np.mean(Data[:,i])\n",
    "        Real_TimeFeature[3, i] = rms(Data[:,i])\n",
    "        Real_TimeFeature[4, i] = np.var(Data[:,i])\n",
    "        Real_TimeFeature[5, i] = sp.skew(Data[:,i])\n",
    "        Real_TimeFeature[6, i] = sp.kurtosis(Data[:,i])\n",
    "        Real_TimeFeature[7, i] = np.max(Data[:,i])/rms(Data[:,i])\n",
    "        Real_TimeFeature[8, i] = rms(Data[:,i])/np.mean(Data[:,i])\n",
    "        Real_TimeFeature[9, i] = np.max(Data[:,i])/np.mean(Data[:,i])\n",
    "\n",
    "    s = 'Real_TimeFeature_%s = Real_TimeFeature'%(Sensor)\n",
    "    exec(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a09521",
   "metadata": {},
   "outputs": [],
   "source": [
    "Real_TimeFeature_Current.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce12d76",
   "metadata": {},
   "source": [
    "# Mahalonobis loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in range(0,3):\n",
    "        Noise_dim = 100*i+100\n",
    "        \n",
    "        try:\n",
    "            save_dir1 = './MahalanobisDistance_NDunify'\n",
    "            os.mkdir(save_dir1)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            save_dir2 = './MahalanobisDistance_NDunify/%s'%(Type)\n",
    "            os.mkdir(save_dir2)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        save_dir3 = './MahalanobisDistance_NDunify/%s/Noise_dim %d/'%(Type,Noise_dim)\n",
    "        os.mkdir(save_dir3)\n",
    "        \n",
    "except FileExistsError:\n",
    "    print(\"File Already Exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoOfFakeData = 300\n",
    "NoOfSensor  = 1\n",
    "NoOfFeature = 10\n",
    "for i in range (0,3):\n",
    "    Noise_dim = 100*i+100\n",
    "    \n",
    "    for j in range(0,3):\n",
    "        Current_LayerNumber = j+1\n",
    "        Sensor = 'Current'\n",
    "        \n",
    "        Current_title = '[%s_%s]'%(Type, Sensor)+'LN'+str(Current_LayerNumber)+'_ND'+str(Noise_dim)\n",
    "        path_dir = 'C:/Users/SDML/SDML challenge/GAN (2)/GAN_DATA/'+Current_title\n",
    "        file_list = os.listdir(path_dir)\n",
    "        \n",
    "        def load_data(dataNum = 2774, fileNum = 300, file_list = file_list):\n",
    "            total_data = np.zeros((dataNum, fileNum))\n",
    "            for a in range(fileNum):\n",
    "                path = path_dir + '/' + file_list[a]\n",
    "                data = pd.read_csv(path, sep=',', header=None).iloc[:,1] # data[:, 0]에 index 존재\n",
    "                total_data[:,a] = data\n",
    "            return total_data\n",
    "                \n",
    "        FakeData = load_data()\n",
    "\n",
    "        Fake_TimeFeature  = np.zeros((NoOfSensor*NoOfFeature , NoOfFakeData))\n",
    "\n",
    "        for k in range(NoOfFakeData):\n",
    "\n",
    "            # Normal Time Domain Feature\n",
    "            Fake_TimeFeature[0, k] = np.max(FakeData[:,k])\n",
    "            Fake_TimeFeature[1, k] = np.min(FakeData[:,k])\n",
    "            Fake_TimeFeature[2, k] = np.mean(FakeData[:,k])\n",
    "            Fake_TimeFeature[3, k] = rms(FakeData[:,k])\n",
    "            Fake_TimeFeature[4, k] = np.var(FakeData[:,k])\n",
    "            Fake_TimeFeature[5, k] = sp.skew(FakeData[:,k])\n",
    "            Fake_TimeFeature[6, k] = sp.kurtosis(FakeData[:,k])\n",
    "            Fake_TimeFeature[7, k] = np.max(FakeData[:,k])/rms(FakeData[:,k])\n",
    "            Fake_TimeFeature[8, k] = rms(FakeData[:,k])/np.mean(FakeData[:,k])\n",
    "            Fake_TimeFeature[9, k] = np.max(FakeData[:,k])/np.mean(FakeData[:,k])\n",
    "\n",
    "        Fake_TimeFeature_Current = Fake_TimeFeature\n",
    "        \n",
    "        for l in range(0,3):\n",
    "            Vol_LayerNumber = l+1\n",
    "            Sensor = 'Vol'\n",
    "\n",
    "            Vol_title = '[%s_%s]'%(Type, Sensor)+'LN'+str(Vol_LayerNumber)+'_ND'+str(Noise_dim)\n",
    "            path_dir = 'C:/Users/SDML/SDML challenge/GAN (2)/GAN_DATA/'+Vol_title     \n",
    "            file_list = os.listdir(path_dir)\n",
    "\n",
    "            FakeData = load_data()\n",
    "\n",
    "\n",
    "            Fake_TimeFeature  = np.zeros((NoOfSensor*NoOfFeature , NoOfFakeData))\n",
    "\n",
    "            for m in range(NoOfFakeData):\n",
    "\n",
    "                # Normal Time Domain Feature\n",
    "                Fake_TimeFeature[0, m] = np.max(FakeData[:,m])\n",
    "                Fake_TimeFeature[1, m] = np.min(FakeData[:,m])\n",
    "                Fake_TimeFeature[2, m] = np.mean(FakeData[:,m])\n",
    "                Fake_TimeFeature[3, m] = rms(FakeData[:,m])\n",
    "                Fake_TimeFeature[4, m] = np.var(FakeData[:,m])\n",
    "                Fake_TimeFeature[5, m] = sp.skew(FakeData[:,m])\n",
    "                Fake_TimeFeature[6, m] = sp.kurtosis(FakeData[:,m])\n",
    "                Fake_TimeFeature[7, m] = np.max(FakeData[:,m])/rms(FakeData[:,m])\n",
    "                Fake_TimeFeature[8, m] = rms(FakeData[:,m])/np.mean(FakeData[:,m])\n",
    "                Fake_TimeFeature[9, m] = np.max(FakeData[:,m])/np.mean(FakeData[:,m])\n",
    "\n",
    "            Fake_TimeFeature_Vol = Fake_TimeFeature\n",
    "            \n",
    "            for n in range(0,3):\n",
    "                Acc_LayerNumber = n+1\n",
    "                Sensor = 'Acc'\n",
    "\n",
    "                Acc_title = '[%s_%s]'%(Type, Sensor)+'LN'+str(Acc_LayerNumber)+'_ND'+str(Noise_dim)\n",
    "                path_dir = 'C:/Users/SDML/SDML challenge/GAN (2)/GAN_DATA/'+Acc_title     \n",
    "                file_list = os.listdir(path_dir)\n",
    "\n",
    "                FakeData = load_data()\n",
    "\n",
    "                Fake_TimeFeature  = np.zeros((NoOfSensor*NoOfFeature , NoOfFakeData))\n",
    "\n",
    "                for o in range(NoOfFakeData):\n",
    "\n",
    "                    # Normal Time Domain Feature\n",
    "                    Fake_TimeFeature[0, o] = np.max(FakeData[:,o])\n",
    "                    Fake_TimeFeature[1, o] = np.min(FakeData[:,o])\n",
    "                    Fake_TimeFeature[2, o] = np.mean(FakeData[:,o])\n",
    "                    Fake_TimeFeature[3, o] = rms(FakeData[:,o])\n",
    "                    Fake_TimeFeature[4, o] = np.var(FakeData[:,o])\n",
    "                    Fake_TimeFeature[5, o] = sp.skew(FakeData[:,o])\n",
    "                    Fake_TimeFeature[6, o] = sp.kurtosis(FakeData[:,o])\n",
    "                    Fake_TimeFeature[7, o] = np.max(FakeData[:,o])/rms(FakeData[:,o])\n",
    "                    Fake_TimeFeature[8, o] = rms(FakeData[:,o])/np.mean(FakeData[:,o])\n",
    "                    Fake_TimeFeature[9, o] = np.max(FakeData[:,o])/np.mean(FakeData[:,o])\n",
    "\n",
    "                Fake_TimeFeature_Acc = Fake_TimeFeature\n",
    "                \n",
    "                MD_input_Current = np.concatenate([Real_TimeFeature_Current,Fake_TimeFeature_Current], axis = 1)\n",
    "\n",
    "                MD_input_Vol = np.concatenate([Real_TimeFeature_Vol,Fake_TimeFeature_Vol], axis = 1)\n",
    "\n",
    "                MD_input_Acc = np.concatenate([Real_TimeFeature_Acc,Fake_TimeFeature_Acc], axis = 1)\n",
    "                \n",
    "                MD_input = np.concatenate([MD_input_Current,MD_input_Vol,MD_input_Acc], axis = 0)\n",
    "                \n",
    "                Normal_Value = pd.DataFrame(MD_input.T)\n",
    "                \n",
    "                cov_mat = Normal_Value[:NoOfData].cov()\n",
    "                inv_cov_mat = scipy.linalg.inv(cov_mat)\n",
    "                average_Value = Normal_Value[:NoOfData].mean()\n",
    "\n",
    "                mah_total = np.zeros((Normal_Value.shape[0], 1))\n",
    "                \n",
    "                for p in np.arange(Normal_Value.shape[0]):\n",
    "                    mah_total[p] = md(Normal_Value.iloc[p, :],average_Value, inv_cov_mat)\n",
    "                mah_total\n",
    "                \n",
    "                title = 'CVA_LayerNumber(%d,%d,%d)'%(Current_LayerNumber,Vol_LayerNumber,Acc_LayerNumber)\n",
    "                pd.DataFrame(mah_total).to_csv('./MahalanobisDistance_NDunify/%s/Noise_dim %d/'%(Type,Noise_dim)+title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a24f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fake_TimeFeature  = np.zeros((NoOfSensor*NoOfFeature , NoOfFakeData))\n",
    "pd.DataFrame(Fake_TimeFeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f78d0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a68b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
